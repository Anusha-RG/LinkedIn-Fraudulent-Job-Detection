# -*- coding: utf-8 -*-
"""CSE291D_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGNWTDZymFRZeiyuRxNgwHpC2xbR9rLf
"""

import os
from bs4 import BeautifulSoup
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
from tabulate import tabulate
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

!pip install beautifulsoup4

import kagglehub

# Download latest version
path = kagglehub.dataset_download("amruthjithrajvr/recruitment-scam")

print("Path to dataset files:", path)

import os

# Print all files in the downloaded dataset directory
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

import pandas as pd

postings_path = '/root/.cache/kagglehub/datasets/amruthjithrajvr/recruitment-scam/versions/3/DataSet.csv'
postings_df = pd.read_csv(postings_path)
print("Initial Recruitment Data:")
print(postings_df.head())
print(postings_df.shape)

# Assuming 'postings_df' is your main DataFrame
print("Job Postings Data:")
print(tabulate(postings_df.head(), headers='keys', tablefmt='psql', showindex=False))

postings_df = postings_df[['description','fraudulent']]
print(postings_df.columns)

postings_df['description'] = postings_df['description'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())
postings_df['fraudulent'] = postings_df['fraudulent'].map({'t': 1, 'f': 0})

postings_df.head()

import nltk
import string
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize
def preprocess_text(text):
    if not isinstance(text, str):
        return ''
    # Tokenize the text
    tokens = word_tokenize(text)

    # Convert tokens to lowercase
    tokens = [word.lower() for word in tokens]

    # Remove punctuation
    tokens = [word for word in tokens if word not in string.punctuation]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Join tokens back into a cleaned sentence
    cleaned_text = ' '.join(tokens)

    return cleaned_text

# Tokenizing dataset columns
postings_df['description'] = postings_df['description'].apply(preprocess_text)

y = postings_df['fraudulent']
postings_df.drop('fraudulent', axis=1, inplace=True)
x = postings_df

# import library
from collections import Counter
import imblearn
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42, replacement=True)

# fit predictor and target varialbe
x_rus, y_rus = rus.fit_resample(x, y)

postings_df = pd.DataFrame(x_rus,columns=x.columns)
postings_df['fraudulent'] = y_rus

print('original dataset shape:', Counter(y))
print('Resample dataset shape', Counter(y_rus))

#Splitting test and train data
x_train, x_test, y_train, y_test = train_test_split(x_rus, y_rus, test_size=0.2, random_state=42)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Flatten x_train and x_test
x_train = x_train.squeeze()
x_test = x_test.squeeze()
vectorizer = TfidfVectorizer()
X_train_transformed = vectorizer.fit_transform(x_train)  # Fit and transform on the training data
X_test_transformed = vectorizer.transform(x_test)        # Transform the test data

model_NB = MultinomialNB()
model_NB.fit(X_train_transformed, y_train)

# Predict using the test data
y_pred_NB = model_NB.predict(X_test_transformed)
# Evaluate the model's performance
report = classification_report(y_test, y_pred_NB, zero_division=1)
# Evaluate the model's performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_NB))
print(report)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Confusion matrix values
conf_matrix = np.array([[146, 36], [29, 136]])

# Plot confusion matrix with new colors
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Purples', xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"], cbar_kws={'shrink': 0.8})
plt.title("Confusion Matrix", fontsize=16)
plt.xlabel("Predicted", fontsize=14)
plt.ylabel("Actual", fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=12)
plt.show()

# Metrics values for the bar plot
categories = ['Class 0', 'Class 1']
precision = [0.83, 0.79]
recall = [0.80, 0.82]
f1_score = [0.82, 0.81]

# Bar plot with updated colors
x = np.arange(len(categories))
width = 0.25

plt.figure(figsize=(10, 6))
plt.bar(x - width, precision, width, label='Precision', color='#8ecae6')
plt.bar(x, recall, width, label='Recall', color='#219ebc')
plt.bar(x + width, f1_score, width, label='F1-Score', color='#023047')

plt.xlabel("Classes", fontsize=14)
plt.ylabel("Scores", fontsize=14)
plt.title("Classification Metrics", fontsize=16)
plt.xticks(x, categories, fontsize=12)
plt.ylim(0, 1)
plt.legend(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import kagglehub

# Download latest version
path = kagglehub.dataset_download("arshkon/linkedin-job-postings")

print("Path to dataset files:", path)

import os

# Print all files in the downloaded dataset directory
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

import pandas as pd

postings_path = '/root/.cache/kagglehub/datasets/arshkon/linkedin-job-postings/versions/13/postings.csv'
lpostings_df_main = pd.read_csv(postings_path)
print("Initial Recruitment Data:")
print(lpostings_df_main.head())
print(lpostings_df_main.shape)

# Assuming 'postings_df' is your main DataFrame
print("Job Postings Data:")
print(tabulate(lpostings_df_main.head(), headers='keys', tablefmt='psql', showindex=False))

lpostings_df = lpostings_df_main[['description', 'location','title']].copy()

# Tokenizing dataset columns
lpostings_df.loc[:, 'description'] = lpostings_df['description'].apply(preprocess_text)

x1=lpostings_df['description']
x1 = x1.fillna('')
print(x1.head())

import numpy as np

# Assuming X_new is very large
batch_size = 10000
all_predictions = []

for i in range(0, len(x1), batch_size):
    batch = x1[i:i+batch_size]
    batch_transformed = vectorizer.transform(batch)
    batch_predictions = model_NB.predict(batch_transformed)
    all_predictions.extend(batch_predictions)

y_new_pred = np.array(all_predictions)
print("Total predictions made:", len(y_new_pred))

lpostings_df['fraudulent'] = y_new_pred
print("New column 'fraudulent' added to lpostings_df")
print("Shape of updated postings_df:", lpostings_df.shape)
 # Display the first few rows to verify
print(lpostings_df[['fraudulent']].head())

lpostings_df.head()

# Filter the dataset for fraudulent and legitimate job descriptions
fraudulent_df = lpostings_df[lpostings_df['fraudulent'] == 1]
legitimate_df = lpostings_df[lpostings_df['fraudulent'] == 0]

# Add a column for job description length in both fraudulent and legitimate job descriptions
fraudulent_df['description_length'] = fraudulent_df['description'].apply(len)
legitimate_df['description_length'] = legitimate_df['description'].apply(len)

# Plot comparison of job description lengths for fraudulent vs legitimate job postings
plt.figure(figsize=(10, 6))

# Plot fraudulent job description lengths
sns.histplot(fraudulent_df['description_length'], kde=True, color='red', label='Fraudulent', stat='density', common_norm=False)

# Plot legitimate job description lengths
sns.histplot(legitimate_df['description_length'], kde=True, color='green', label='Legitimate', stat='density', common_norm=False)

# Add titles and labels
plt.title('Comparison of Job Description Lengths: Fraudulent vs Legitimate Job Postings')
plt.xlabel('Job Description Length')
plt.ylabel('Density')
plt.legend()

plt.show()

fraudulent_jobs = lpostings_df[lpostings_df['fraudulent'] == 1]
# Add new columns for title and description lengths
fraudulent_jobs['title_length'] = fraudulent_jobs['title'].apply(len)
fraudulent_jobs['description_length'] = fraudulent_jobs['description'].apply(len)
import re
from collections import Counter

# List of scam-related characteristics and their corresponding keywords/phrases
scam_characteristics = {
    "Suspicious Contact Information": ["contact us", "phone", "email", "apply now"],
    "Unrealistic Salary Offers": ["$1000/day", "high pay", "earn big", "get rich quick"],
    "Misleading Job Descriptions": ["no experience", "start immediately", "part-time hours"],
    "Promises of Easy Money": ["easy money", "guaranteed income", "quick cash"],
    "Upfront Payment Requests": ["application fee", "training fee", "deposit required"],
    "Urgency in Application Processes": ["apply immediately", "urgent", "limited spots"],
    "Vague Company Details": ["unknown company", "confidential", "no details provided"],
    "Unprofessional Language": ["typos", "slang", "all caps", "bad grammar"],
    "Pressure to Act Quickly": ["limited offer", "last chance", "hurry up"],
}

# Function to count scam-related keywords in a given text
def count_scam_keywords(text, characteristics):
    counts = {key: 0 for key in characteristics.keys()}
    if isinstance(text, str):
        for characteristic, keywords in characteristics.items():
            counts[characteristic] = sum(
                1 for keyword in keywords if re.search(r'\b' + re.escape(keyword) + r'\b', text, re.IGNORECASE)
            )
    return counts

# Apply the function to the fraudulent job descriptions and titles
fraudulent_jobs['scam_keyword_counts'] = fraudulent_jobs.apply(
    lambda row: count_scam_keywords(row['description'] + " " + row['title'], scam_characteristics), axis=1
)

# Sum the counts for each scam characteristic
scam_characteristics_summary = Counter()
for counts in fraudulent_jobs['scam_keyword_counts']:
    scam_characteristics_summary.update(counts)

# Prepare data for visualization
characteristics = list(scam_characteristics_summary.keys())
counts = list(scam_characteristics_summary.values())

# Plot the characteristics
plt.figure(figsize=(12, 6))
sns.barplot(x=counts, y=characteristics, palette='coolwarm')
plt.title('Top Job Scam Characteristics')
plt.xlabel('Frequency')
plt.ylabel('Scam Characteristics')
plt.show()

from wordcloud import WordCloud

# Combine all fraudulent descriptions into one text
all_fraud_descriptions = " ".join(fraudulent_jobs['description'].dropna())

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='coolwarm').generate(all_fraud_descriptions)

# Display the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Common Words in Fraudulent Job Descriptions')
plt.show()

# Regular expressions for contact information
email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
phone_pattern = r'\+?\d[\d -]{8,12}\d'  # Matches phone numbers with various formats

# Count the occurrences of email and phone in descriptions
fraudulent_jobs['has_email'] = fraudulent_jobs['description'].apply(lambda x: bool(re.search(email_pattern, str(x))))
fraudulent_jobs['has_phone'] = fraudulent_jobs['description'].apply(lambda x: bool(re.search(phone_pattern, str(x))))

# Count the rows with these characteristics
contact_info_summary = fraudulent_jobs[['has_email', 'has_phone']].sum().reset_index()
contact_info_summary.columns = ['Contact Info Type', 'Frequency']

# Plot the presence of contact info
plt.figure(figsize=(8, 5))
sns.barplot(x='Contact Info Type', y='Frequency', data=contact_info_summary, palette='coolwarm')
plt.title('Presence of Suspicious Contact Information in Fraudulent Jobs')
plt.xlabel('Contact Info Type')
plt.ylabel('Frequency')
plt.show()

from itertools import chain
# Define a regex pattern for phone numbers
phone_pattern = r'\+?\d[\d\s.-]{7,14}\d'

# Extract phone numbers from job descriptions
fraudulent_jobs['phone_numbers'] = fraudulent_jobs['description'].apply(
    lambda x: re.findall(phone_pattern, str(x))
)

# Flatten the list of phone numbers and count occurrences
phone_counts = Counter(chain(*fraudulent_jobs['phone_numbers']))
phone_df = pd.DataFrame(phone_counts.items(), columns=['Phone Number', 'Frequency']).sort_values(by='Frequency', ascending=False)

# Plot the top 10 most frequent phone numbers
plt.figure(figsize=(10, 6))
sns.barplot(x='Frequency', y='Phone Number', data=phone_df.head(10), palette='coolwarm')
plt.title('Top 10 Phone Numbers in Fraudulent Job Descriptions')
plt.xlabel('Frequency')
plt.ylabel('Phone Number')
plt.show()

from collections import Counter
import re
import seaborn as sns
import matplotlib.pyplot as plt

# Example scam-related keywords
scam_keywords = ['urgent', 'easy money', 'no experience', 'work from home', 'guaranteed']

# Tokenize job descriptions and count occurrences of scam keywords
def count_keywords(job_description, keywords):
    if isinstance(job_description, str):  # Check if it's a string
        count = {keyword: 0 for keyword in keywords}  # Initialize counts for each keyword
        for keyword in keywords:
            if re.search(r'\b' + re.escape(keyword) + r'\b', job_description, re.IGNORECASE):
                count[keyword] += 1
        return count
    else:
        return {keyword: 0 for keyword in keywords}  # Return 0 for non-string values or NaN

# Filter the dataset to include only fraudulent job descriptions (fraudulent = 1)
fraudulent_df = lpostings_df[lpostings_df['fraudulent'] == 1]

# Initialize a dictionary to count occurrences of each scam keyword
keyword_counts = {keyword: 0 for keyword in scam_keywords}

# Apply the function to the description column of fraudulent job descriptions and count keyword occurrences
fraudulent_df['scam_keywords_count'] = fraudulent_df['description'].apply(lambda x: count_keywords(str(x), scam_keywords))

# Sum the counts for each keyword across all fraudulent job descriptions
for index, row in fraudulent_df.iterrows():
    for keyword in scam_keywords:
        keyword_counts[keyword] += row['scam_keywords_count'][keyword]

# Prepare the data for plotting
keyword_labels = list(keyword_counts.keys())
keyword_values = list(keyword_counts.values())

# Plot the bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=keyword_labels, y=keyword_values, palette='coolwarm')
plt.title('Frequency of Scam Keywords in Fraudulent Job Descriptions')
plt.xlabel('Scam Keywords')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt

# Exclude "United States" from the DataFrame
filtered_postings_df = lpostings_df[lpostings_df['location'] != 'United States']

# Group by location and fraudulent column, then count postings
fraudulent_counts = filtered_postings_df.groupby(['location', 'fraudulent']).size().unstack(fill_value=0)

# Rename columns for clarity
fraudulent_counts.columns = ['Legitimate', 'Fake']

# Sort by total postings (Legitimate + Fake) in descending order
fraudulent_counts['Total'] = fraudulent_counts['Legitimate'] + fraudulent_counts['Fake']
fraudulent_counts = fraudulent_counts.sort_values('Total', ascending=False)

# Display the top 10 locations with the most postings
top_locations = fraudulent_counts.head(10)
print(top_locations)

# Plot the data
top_locations[['Legitimate', 'Fake']].plot(kind='barh', stacked=True, figsize=(10, 7))
plt.title("Legitimate vs. Fake Job Postings by Location (Top 10)")
plt.xlabel("Number of Postings")
plt.ylabel("Location")
plt.legend(title="Posting Type")
plt.show()

import matplotlib.pyplot as plt

# Filter the dataset for fraudulent job postings
fake_jobs = lpostings_df[lpostings_df['fraudulent'] == 1]

# Count the occurrences of each title in fraudulent postings
fake_roles_counts = fake_jobs['title'].value_counts()

# Select the top 10 most common roles targeted
top_fake_roles = fake_roles_counts.head(10)

# Plot the data
plt.figure(figsize=(10, 7))
top_fake_roles.plot(kind='barh', color='salmon')
plt.title("Top Roles Targeted for Fake Jobs")
plt.xlabel("Number of Fake Job Postings")
plt.ylabel("Job Role")
plt.gca().invert_yaxis()  # Invert y-axis to show the highest count at the top
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

y = lpostings_df['fraudulent']
lpostings_df.drop('fraudulent', axis=1, inplace=True)
x = lpostings_df[['description']]

# import library
from collections import Counter
import imblearn
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42, replacement=True)

# fit predictor and target varialbe
x_rus, y_rus = rus.fit_resample(x, y)

postings_df = pd.DataFrame(x_rus,columns=x.columns)
postings_df['fraudulent'] = y_rus

print('original dataset shape:', Counter(y))
print('Resample dataset shape', Counter(y_rus))

#Splitting test and train data
x_train, x_test, y_train, y_test = train_test_split(x_rus, y_rus, test_size=0.2, random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Flatten x_train and x_test
x_train = x_train.squeeze()
x_test = x_test.squeeze()

# Initialize and fit TfidfVectorizer
vectorizer = TfidfVectorizer()
X_train_transformed = vectorizer.fit_transform(x_train)  # Fit and transform on the training data
X_test_transformed = vectorizer.transform(x_test)        # Transform the test data

# Initialize and train Logistic Regression model
model_LR = LogisticRegression(random_state=42, max_iter=1000)
model_LR.fit(X_train_transformed, y_train)

# Predict using the test data
y_pred_LR = model_LR.predict(X_test_transformed)

# Evaluate the model's performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_LR))

print("\nClassification Report:")
report = classification_report(y_test, y_pred_LR, zero_division=1)
print(report)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Assuming vectorizer and model_LR have been trained on your dataset
# If not, you'll need to train them first

# Sample job description
sample_description = ["""Here are some details about the role.Position: Software Engineer Location: San Diego, CA
Duration: 6+ Months

Software Engineer (Mid)

Responsibilities:
Manage migrations of users to a new IAM permissions system
Troubleshoot Python scripts, API, or AWS if issues arise during migrations
Be able to explain technical system at a high level to users
Responed and work on Customer requests through multiple channels, such as slack, service desk requests.


Qualifications:
Github version control experience (approx.. 2-3 yrs)
Intermediate level of Python coding experience (approx.. 2-3 yrs)
AWS IAM experience (approx.. 2-3 yrs)
Enterprise-level experience
Experience with AWS Serverless services including AWS Lambda
Cloud Security background is a plus




If you're interested, let's connect for a quick chat here or you can text me at 408-863-1191."""]

# Transform the job description using the trained vectorizer
sample_transformed = vectorizer.transform(sample_description)

# Predict using the trained model
prediction = model_LR.predict(sample_transformed)

# Get the probability scores
probability_scores = model_LR.predict_proba(sample_transformed)

# Print the prediction and probability scores
print(f"The model predicts that the job posting is: {prediction[0]}")
print(f"Probability scores: {probability_scores[0]}")

# If you want to get the class labels (assuming binary classification)
class_labels = model_LR.classes_
for label, prob in zip(class_labels, probability_scores[0]):
    print(f"Probability of being {label}: {prob:.4f}")

"""Anlaysis to be made:
1. Does job location affect scam risk? Are fake job postings concentrated in certain locations?
2. Which roles are most vulnerable?
3. What are the top job scam characteristics?
Suspicious Contact Information: 41.1%
Unrealistic Salary Offers: 25.7%
Misleading Job Descriptions: 10.6%
Promises of Easy Money: 10.1%
Upfront Payment Requests: 7.3%
Unpaid Work: 0.9%
Urgency in Application Processes: 0.9%
Vague Company Details: 0.2%
Unprofessional Language: 2.8%
Pressure to Act Quickly: 0.4%
"""

# Exclude 'United States' from location counts
filtered_locations = lpostings_df_main[lpostings_df_main['location'] != 'United States']

# Get the top 10 locations after filtering
location_counts = filtered_locations['location'].value_counts().head(10)

# Plot the data
sns.barplot(y=location_counts.index, x=location_counts.values)
plt.title("Top Locations for Job Postings")
plt.xlabel("Number of Postings")
plt.ylabel("Location")
plt.show()

# Convert 'zip_code' to string and check for anomalies
lpostings_df_main['zip_code'] = lpostings_df_main['zip_code'].astype(str)

# Identify anomalies in zip codes
anomalous_zip_codes = lpostings_df_main[lpostings_df_main['zip_code'].isna() | (lpostings_df_main['zip_code'].str.len() != 5)]
print(anomalous_zip_codes[['title', 'zip_code', 'company_name']])